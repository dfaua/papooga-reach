import { NextRequest, NextResponse } from "next/server";
import { validateApiKey } from "@/app/lib/auth/api-key";
import { supabase } from "@/app/lib/supabase/client";

const GEMINI_API_KEY = process.env.GEMINI_API_KEY;
const GROQ_API_KEY = process.env.GROQ_API_KEY;

interface ConversationMessage {
  direction: "sent" | "received";
  content: string;
  created_at: string;
  platform?: string;
}

interface SuggestRequest {
  personId: string;
  conversationHistory: ConversationMessage[];
  userDraft?: string;
  mode: "generate" | "polish";
  model?: string;
  messageInstructions?: string;
}

// Available models configuration
const MODELS: Record<string, { provider: string; modelId: string }> = {
  "gemini/gemini-3-pro-preview": {
    provider: "gemini",
    modelId: "gemini-3-pro-preview",
  },
  "gemini/gemini-3-flash-preview": {
    provider: "gemini",
    modelId: "gemini-3-flash-preview",
  },
  "groq/llama-3.3-70b": {
    provider: "groq",
    modelId: "llama-3.3-70b-versatile",
  },
  "groq/llama-4-scout": {
    provider: "groq",
    modelId: "llama-4-scout-17b-16e-instruct",
  },
};

async function callGemini(systemPrompt: string, userPrompt: string, modelId: string): Promise<string> {
  const response = await fetch(
    `https://generativelanguage.googleapis.com/v1beta/models/${modelId}:generateContent?key=${GEMINI_API_KEY}`,
    {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
      },
      body: JSON.stringify({
        contents: [
          {
            role: "user",
            parts: [{ text: systemPrompt + "\n\n" + userPrompt }],
          },
        ],
        generationConfig: {
          temperature: 0.7,
          maxOutputTokens: 4096,
        },
      }),
    }
  );

  if (!response.ok) {
    const errorText = await response.text();
    console.error("Gemini API error:", errorText);
    throw new Error(`Gemini API error: ${response.status}`);
  }

  const data = await response.json();
  const generatedText = data.candidates?.[0]?.content?.parts?.[0]?.text;

  if (!generatedText) {
    console.error("No text in Gemini response:", data);
    throw new Error("No text generated by AI");
  }

  return generatedText;
}

async function callGroq(systemPrompt: string, userPrompt: string, modelId: string): Promise<string> {
  const response = await fetch("https://api.groq.com/openai/v1/chat/completions", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
      "Authorization": `Bearer ${GROQ_API_KEY}`,
    },
    body: JSON.stringify({
      model: modelId,
      messages: [
        { role: "system", content: systemPrompt },
        { role: "user", content: userPrompt },
      ],
      temperature: 0.7,
      max_tokens: 4096,
    }),
  });

  if (!response.ok) {
    const errorText = await response.text();
    console.error("Groq API error:", errorText);
    throw new Error(`Groq API error: ${response.status}`);
  }

  const data = await response.json();
  const generatedText = data.choices?.[0]?.message?.content;

  if (!generatedText) {
    console.error("No text in Groq response:", data);
    throw new Error("No text generated by AI");
  }

  return generatedText;
}

export async function POST(request: NextRequest) {
  const authError = validateApiKey(request);
  if (authError) return authError;

  const body: SuggestRequest = await request.json();
  const { personId, conversationHistory, userDraft, mode, model, messageInstructions } = body;

  // Fetch person info
  const { data: person, error: personError } = await supabase
    .from("people")
    .select("name, title, company_name")
    .eq("id", personId)
    .single();

  if (personError || !person) {
    return NextResponse.json(
      { error: "Person not found" },
      { status: 404 }
    );
  }

  // Fetch AI settings (using type assertion since table may not be in generated types yet)
  // eslint-disable-next-line @typescript-eslint/no-explicit-any
  const { data: aiSettings } = await (supabase as any)
    .from("ai_settings")
    .select("company_info, custom_instructions")
    .limit(1)
    .single();

  const companyInfo = aiSettings?.company_info || "";
  const customInstructions = aiSettings?.custom_instructions || "";

  // Get model config, default to gemini-3-pro-preview
  const modelKey = model || "gemini/gemini-3-pro-preview";
  const modelConfig = MODELS[modelKey];

  if (!modelConfig) {
    return NextResponse.json(
      { error: `Unknown model: ${model}` },
      { status: 400 }
    );
  }

  // Build system prompt
  let systemPrompt = `You are a LinkedIn messaging assistant helping write professional messages.`;

  if (companyInfo) {
    systemPrompt += `\n\nAbout my company:\n${companyInfo}`;
  }

  if (customInstructions) {
    systemPrompt += `\n\nGeneral instructions:\n${customInstructions}`;
  }

  if (messageInstructions) {
    systemPrompt += `\n\n**IMPORTANT - Instructions for this specific message (these override general instructions if there's a conflict):**\n${messageInstructions}`;
  }

  systemPrompt += `\n\nRules:
1. Keep messages concise and professional but friendly (unless instructed otherwise)
2. Be natural and conversational - avoid sounding robotic
3. Return ONLY the message text, no explanations or commentary
4. Don't use generic phrases like "I hope this finds you well"
5. Keep messages appropriate for LinkedIn messaging
6. If per-message instructions conflict with general instructions, follow the per-message instructions`;

  // Build user prompt with conversation context
  let userPrompt = `Conversation with ${person.name} (${person.title || "Unknown role"} at ${person.company_name || "Unknown company"}):\n\n`;

  // Add conversation history
  if (conversationHistory.length > 0) {
    userPrompt += "Previous messages:\n";
    for (const msg of conversationHistory) {
      const sender = msg.direction === "sent" ? "Me" : person.name;
      userPrompt += `${sender}: ${msg.content}\n\n`;
    }
  } else {
    userPrompt += "(No previous messages - this is the start of a new conversation)\n\n";
  }

  // Add task based on mode
  if (mode === "polish" && userDraft) {
    userPrompt += `My draft message (please polish and improve this while keeping the core meaning):\n"""${userDraft}"""\n\nPlease polish this message to make it more professional and effective. Keep the same intent but improve the wording.`;
  } else {
    userPrompt += `Please generate a natural, professional reply that continues this conversation.`;
  }

  try {
    let generatedText: string;

    if (modelConfig.provider === "gemini") {
      generatedText = await callGemini(systemPrompt, userPrompt, modelConfig.modelId);
    } else if (modelConfig.provider === "groq") {
      generatedText = await callGroq(systemPrompt, userPrompt, modelConfig.modelId);
    } else {
      throw new Error(`Unknown provider: ${modelConfig.provider}`);
    }

    return NextResponse.json({ suggestion: generatedText.trim() });
  } catch (error) {
    console.error("AI suggest-message error:", error);
    return NextResponse.json(
      { error: error instanceof Error ? error.message : "Failed to generate message" },
      { status: 500 }
    );
  }
}

// GET endpoint to list available models
export async function GET(request: NextRequest) {
  const authError = validateApiKey(request);
  if (authError) return authError;

  return NextResponse.json({
    models: Object.keys(MODELS),
    default: "gemini/gemini-3-pro-preview",
  });
}
